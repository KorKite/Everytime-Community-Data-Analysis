{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 머신러닝 기반 분류기\n",
    "머신러닝으로 이를 학습하고 예측하는 모델이다.\n",
    "tfidf - SVC 와 CountVec - MultinomialNB 두 조합이 사용되었다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 불러오고 전처리\n",
    "whole = 전체 데이터\n",
    "df = test용 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "whole = pd.read_csv(\"data.csv\")\n",
    "df = pd.read_csv(\"tmp.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "별점\n",
       "1     734\n",
       "2     503\n",
       "3    1134\n",
       "4    1346\n",
       "5    1784\n",
       "dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whole.groupby(whole['별점']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5501"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(whole)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "okt = Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = okt.pos(text, norm = True, stem = True)\n",
    "    tmp = []\n",
    "    conti = False\n",
    "    for i in text:           \n",
    "        if  i[1] == \"Adjective\" or i[1] == \"Noun\" or i[1] == \"Verb\" or i[1] == \"Adjective\" or i[1] == \"Suffix\" or i[1] ==\"KoreanParticle\":\n",
    "            tmp.append(i[0])\n",
    "            prev = (i[0], i[1])\n",
    "        \n",
    "    \n",
    "    return \" \".join(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole[\"전처리\"] = whole[\"강의평\"].map(clean_text)\n",
    "df[\"전처리\"] = df[\"강의평\"].map(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# whole_pn = whole[whole[\"긍정부정\"] ==\"neg\"].append(whole[whole[\"긍정부정\"] ==\"pos\"])\n",
    "# df_pn = df[df[\"긍정부정\"] ==\"neg\"].append(df[df[\"긍정부정\"] ==\"pos\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = list(whole[\"전처리\"])\n",
    "train_sent = list(whole[\"별점\"])\n",
    "test_text = list(df[\"전처리\"])\n",
    "test_sent = list(df[\"별점\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tfidf와 SVC를 사용한 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=10000)\n",
    "term_docs_train = tfidf_vectorizer.fit_transform(train_text)\n",
    "term_docs_test = tfidf_vectorizer.transform(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "          intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "          multi_class='ovr', penalty='l2', random_state=42, tol=0.0001,\n",
       "          verbose=0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "svm = LinearSVC(C= 1.0, penalty = \"l2\", random_state= 42)\n",
    "svm.fit(term_docs_train, train_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.53      0.34      0.42        29\n",
      "           2       0.13      0.14      0.14        14\n",
      "           3       0.48      0.26      0.34        42\n",
      "           4       0.36      0.41      0.38        32\n",
      "           5       0.26      0.61      0.37        18\n",
      "\n",
      "    accuracy                           0.35       135\n",
      "   macro avg       0.35      0.35      0.33       135\n",
      "weighted avg       0.40      0.35      0.35       135\n",
      "\n",
      "accuracy: 34.815 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "def evaluate(test_x, test_y, model):\n",
    "    predictions = model.predict(test_x)\n",
    "    print(classification_report(test_y, predictions))\n",
    "#     print(\"Macro F1 score - {0:0.5f}\".format(classification_report(test_y, predictions, output_dict=True)[\"macro avg\"][\"f1-score\"]))\n",
    "evaluate(term_docs_test, test_sent, svm)\n",
    "accuracy = svm.score(term_docs_test, test_sent)\n",
    "\n",
    "print(\"accuracy: %0.3f\"%(accuracy*100),\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVectorizer와 MultinomialNB를 사용한 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "CV = CountVectorizer(lowercase=True, max_df=1.0)\n",
    "term_docs_train_s = CV.fit_transform(train_text)\n",
    "term_docs_test_s = CV.transform(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=0.25, class_prior=None, fit_prior=False)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#최적의 결과를 바탕으로 설정해줌\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nb = MultinomialNB(alpha = 0.25, fit_prior=False)\n",
    "nb.fit(term_docs_train_s, train_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.71      0.52      0.60        29\n",
      "           2       0.29      0.29      0.29        14\n",
      "           3       0.36      0.21      0.27        42\n",
      "           4       0.34      0.53      0.41        32\n",
      "           5       0.36      0.50      0.42        18\n",
      "\n",
      "    accuracy                           0.40       135\n",
      "   macro avg       0.41      0.41      0.40       135\n",
      "weighted avg       0.42      0.40      0.40       135\n",
      "\n",
      "accuracy: 40.000 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "def evaluate(test_x, test_y, model):\n",
    "    predictions = model.predict(test_x)\n",
    "    print(classification_report(test_y, predictions))\n",
    "#     print(\"Macro F1 score - {0:0.5f}\".format(classification_report(test_y, predictions, output_dict=True)[\"macro avg\"][\"f1-score\"]))\n",
    "evaluate(term_docs_test_s, test_sent, nb)\n",
    "accuracy = nb.score(term_docs_test_s, test_sent)\n",
    "\n",
    "print(\"accuracy: %0.3f\"%(accuracy*100),\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import plot_importance\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBClassifier(n_estimators= 500, learning_rate= 0.1, max_depth = 4)\n",
    "xgb.fit(term_docs_train, train_sent)\n",
    "xgb_pred = xgb.predict(term_docs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.71      0.41      0.52        29\n",
      "           2       0.14      0.07      0.10        14\n",
      "           3       0.39      0.31      0.35        42\n",
      "           4       0.30      0.34      0.32        32\n",
      "           5       0.22      0.50      0.31        18\n",
      "\n",
      "    accuracy                           0.34       135\n",
      "   macro avg       0.35      0.33      0.32       135\n",
      "weighted avg       0.39      0.34      0.35       135\n",
      "\n",
      "accuracy: 34.074 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(test_sent, xgb_pred))\n",
    "accuracy = xgb.score(term_docs_test, test_sent)\n",
    "print(\"accuracy: %0.3f\"%(accuracy*100),\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 위에 report는 완벽하게 이 모델의 성능을 증명하지 못한다.\n",
    "\n",
    "왜냐하면 5점이여야 하는 것을 4점으로 평가하든 1점으로 평가하든 모두 틀렸다고만 평가할 수 있기 때문이다.\n",
    "\n",
    "그러므로 제대로된 평가를 하려면 실제 라벨링된 것과 예측한 라벨링의 오차(cost)가 중요하다. 그러므로 오차값의 평균으로 오차를 평가한다.\n",
    "\n",
    "cost = avg(abs(real - estimated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4996982167352535"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean = 0\n",
    "for i in range(len(test_sent)):\n",
    "    mean += abs(test_sent[i]- nb.predict(term_docs_test_s))\n",
    "\n",
    "(mean/len(test_sent)).sum()/len(test_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5866666666666667"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean = 0\n",
    "for i in range(len(test_sent)):\n",
    "    mean += abs(test_sent[i]- svm.predict(term_docs_test))\n",
    "\n",
    "(mean/len(test_sent)).sum()/len(test_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5458984910836762"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean = 0\n",
    "for i in range(len(test_sent)):\n",
    "    mean += abs(test_sent[i]- xgb.predict(term_docs_test))\n",
    "\n",
    "(mean/len(test_sent)).sum()/len(test_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이를 통해 일반적으로 데이터를 넣었을 때, 좋은 성능에서 1.5점 정도의 오차값을 보인다고 할 수 있다. 즉 실제로 4점인 데이터를 넣었을 때, 2.5 ~ 5점 으로 예측한다는 것이다. 더 많은 데이터를 학습시키면 그 격차는 줄 것이다. 딥러닝을 사용한 모델은 훨씬 더 정확하게 예측하였다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 실제 데이터를 넣어서 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [\"아 개같은 수업 못들어 먹겠다.\"]\n",
    "a_test = tfidf_vectorizer.transform(a)\n",
    "nb.predict(a_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [\"정말 최고의 수업입니다.\"]\n",
    "a_test = tfidf_vectorizer.transform(a)\n",
    "nb.predict(a_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
